{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random as pyrandom\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from tensorflow.keras import *\n",
        "\n",
        "\n",
        "# 설정\n",
        "# 데이터셋 파일\n",
        "FILE_PATH = \"all_dataset.csv\"\n",
        "\n",
        "# 증강 배수 설정\n",
        "NORMAL_AUG = 4\n",
        "FALLING_AUG = 6\n",
        "\n",
        "# 학습 관련 설정\n",
        "EARLY_STOPPING_PATIENCE = 50\n",
        "EPOCHS= 50\n",
        "BATCH_SIZE= 32\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "# 기타 하이퍼파라미터\n",
        "THRESHOLD = 0.5  # 예측시 확률 임계값\n",
        "\n",
        "\n",
        "# 모델\n",
        "def build_model():\n",
        "    model = models.Sequential([\n",
        "        layers.InputLayer(shape=(250, 13)),\n",
        "        layers.Conv1D(32, kernel_size=20, padding=\"same\", activation=\"relu\"),\n",
        "        layers.MaxPooling1D(pool_size=2),\n",
        "        layers.Conv1D(64, kernel_size=10, padding=\"same\", activation=\"relu\"),\n",
        "        layers.MaxPooling1D(pool_size=2),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Bidirectional(layers.LSTM(64, return_sequences=False)),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(LEARNING_RATE),\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\", \"AUC\"])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "kSQ-alsqkMKu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 각종 함수들(이것도 꼭 실행해줘야함!)\n",
        "def load_dataset(file):\n",
        "    df = pd.read_csv(file)\n",
        "    df[\"label\"] = df[\"label\"].apply(lambda x: 1 if x == 1 else 0)\n",
        "\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    feature_cols = [col for col in df.columns if col.startswith(\"v\")]\n",
        "\n",
        "    X_all = df[feature_cols].values\n",
        "    y_all = df[\"label\"].values\n",
        "\n",
        "    return X_all, y_all\n",
        "\n",
        "def split_balanced_val(X, y, val_ratio=0.1):\n",
        "    idx_normal = np.where(y == 0)[0]\n",
        "    idx_falling = np.where(y == 1)[0]\n",
        "\n",
        "    np.random.shuffle(idx_normal)\n",
        "    np.random.shuffle(idx_falling)\n",
        "\n",
        "    val_len = int(len(y) * val_ratio)\n",
        "\n",
        "    n = val_len // 2\n",
        "\n",
        "    val_idx = np.concatenate((idx_normal[:n], idx_falling[:n]))\n",
        "    train_idx = np.concatenate((idx_normal[n:], idx_falling[n:]))\n",
        "\n",
        "    np.random.shuffle(val_idx)\n",
        "    np.random.shuffle(train_idx)\n",
        "\n",
        "    return X[train_idx], y[train_idx], X[val_idx], y[val_idx]\n",
        "\n",
        "# 센서 값 위치를 정하면 정해진 윈도우 크기의 앞 센서값을 불러오는 함수\n",
        "def get_window(v, size, end):\n",
        "    return v[end - size + 1 : end + 1]\n",
        "\n",
        "# F0: V(t) - median (window: 50)\n",
        "def calculate_Detrend(v):\n",
        "    output = np.zeros(250)\n",
        "\n",
        "    for t in range(250):\n",
        "       t_in = t + 50\n",
        "       window = get_window(v, 50, t_in)\n",
        "\n",
        "       output[t] = v[t_in] - np.median(window)\n",
        "\n",
        "    return output\n",
        "\n",
        "# F1: MovingMAD (window: 50)\n",
        "def calculate_MovingMAD(v):\n",
        "    output = np.zeros(250)\n",
        "\n",
        "    for t in range(250):\n",
        "        t_in = t + 50\n",
        "        window = get_window(v, 50, t_in)\n",
        "\n",
        "        # MAD 계산\n",
        "        abs_deviations = np.abs(window - np.median(window))\n",
        "        mad = np.median(abs_deviations)\n",
        "\n",
        "        output[t] = mad\n",
        "\n",
        "    return output\n",
        "\n",
        "# F2: MovingKurtosis (window: 50)\n",
        "def calculate_MovingKurtosis(v):\n",
        "    output = np.zeros(250)\n",
        "\n",
        "    for t in range(250):\n",
        "        t_in = t + 50\n",
        "        window = get_window(v, 50, t_in)\n",
        "\n",
        "        # Kurtosis 계산\n",
        "        kurtosis = np.sum((((window - np.mean(window)) / np.std(window))**4) / len(window)) - 3\n",
        "\n",
        "        output[t] = kurtosis\n",
        "\n",
        "    return output\n",
        "\n",
        "# F3: 미분값(기울기)\n",
        "def calculate_Gradient(v):\n",
        "    output = np.asarray(v, dtype=float)\n",
        "    output = output[50:300] - output[49:299]\n",
        "\n",
        "    return output\n",
        "\n",
        "# F4: 창적분값 (window: 15)\n",
        "def calculate_Integral(v):\n",
        "    output = np.asarray(v, dtype=float)\n",
        "    c = np.cumsum(output)\n",
        "    sum = c[50:300] - c[35:285]\n",
        "\n",
        "    return sum\n",
        "\n",
        "# F5~F12: STFT (window: 50)\n",
        "def calculate_STFT(v):\n",
        "    n_fft = 50\n",
        "    STFT_BANDS = [(1, 2), (2, 3), (3, 4), (4, 5),\n",
        "                  (5, 6), (6, 7), (7, 8), (8, 15)]\n",
        "\n",
        "    stft = librosa.stft(y=v.astype(float), n_fft=n_fft, win_length=50, hop_length=1, center=False)\n",
        "    mag = np.abs(stft)\n",
        "    mag = mag[:, 1:251]\n",
        "\n",
        "    # 라이브러리에서 나온 각 밴드가 실제 출력 밴드 어디에 해당하는지 계산\n",
        "    fft_freqs = librosa.fft_frequencies(sr=50, n_fft=n_fft)\n",
        "    band_indices_list = []\n",
        "    for f_min, f_max in STFT_BANDS:\n",
        "        indices = np.where((fft_freqs >= f_min) & (fft_freqs < f_max))[0]\n",
        "        band_indices_list.append(indices)\n",
        "\n",
        "    output_map = np.zeros((250, len(STFT_BANDS)), dtype=np.float32)\n",
        "\n",
        "    for i, band_indices in enumerate(band_indices_list):\n",
        "        if len(band_indices) == 0:\n",
        "            continue\n",
        "        band_energy = mag[band_indices, :].sum(axis=0)\n",
        "        output_map[:, i] = band_energy\n",
        "\n",
        "    return output_map\n",
        "\n",
        "# (300,) 의 데이터를 받고 (250, 13) 데이터로 바꾸는 처리 함수\n",
        "def preprocess_data(v):\n",
        "    feature_map = np.zeros((250, 13))\n",
        "\n",
        "    feature_map[:, 0] = calculate_Detrend(v)\n",
        "    feature_map[:, 1] = calculate_MovingMAD(v)\n",
        "    feature_map[:, 2] = calculate_MovingKurtosis(v)\n",
        "    feature_map[:, 3] = calculate_Gradient(v)\n",
        "    feature_map[:, 4] = calculate_Integral(v)\n",
        "    feature_map[:, 5:] = calculate_STFT(v)\n",
        "\n",
        "    return feature_map\n",
        "\n",
        "def create_feature(dataset):\n",
        "    n_data = dataset.shape[0]\n",
        "\n",
        "    X_processed = np.zeros((n_data, 250, 13))  # (N, 250, 13)의 빈 3D 텐서 생성\n",
        "    for i in range(n_data):\n",
        "        v = dataset[i, :]\n",
        "        X_processed[i, :, :] = preprocess_data(v)\n",
        "\n",
        "    return X_processed\n",
        "\n",
        "def scale(X_train, X_val, X_test):\n",
        "    scaler = RobustScaler()\n",
        "\n",
        "    train_shape = X_train.shape\n",
        "    val_shape = X_val.shape\n",
        "    test_shape = X_test.shape\n",
        "\n",
        "    X_train_flat = X_train.reshape(-1, 13)\n",
        "    X_val_flat = X_val.reshape(-1, 13)\n",
        "    X_test_flat = X_test.reshape(-1, 13)\n",
        "\n",
        "    scaler.fit(X_train_flat)\n",
        "\n",
        "    X_train_scaled = scaler.transform(X_train_flat).reshape(train_shape)\n",
        "    X_val_scaled = scaler.transform(X_val_flat).reshape(val_shape)\n",
        "    X_test_scaled = scaler.transform(X_test_flat).reshape(test_shape)\n",
        "\n",
        "    return X_train_scaled, X_val_scaled, X_test_scaled, scaler\n",
        "\n",
        "def make_spike_noise(v_data):\n",
        "    v_new = np.array(v_data, dtype=int)\n",
        "    data_len = len(v_new)\n",
        "\n",
        "    # 무작위 위치 선택\n",
        "    duration = pyrandom.randint(1, 3)\n",
        "    start_idx = pyrandom.randint(0, data_len - duration)\n",
        "    end_idx = start_idx + duration\n",
        "\n",
        "    if pyrandom.random() < 0.5:  # 하한 스파이크\n",
        "        value = pyrandom.randint(1, 10)\n",
        "    else:  # 상한 스파이크\n",
        "        value = pyrandom.randint(2900,3000)\n",
        "\n",
        "\n",
        "    v_new[start_idx:end_idx] = value\n",
        "\n",
        "    return v_new\n",
        "\n",
        "def make_gaussian_noise(v_data):\n",
        "    v_new = np.array(v_data, dtype=float)\n",
        "\n",
        "    # 가우시안 노이즈 생성\n",
        "    noise = np.random.normal(scale=10.0, size=v_new.shape)\n",
        "    v_new = v_new + noise\n",
        "\n",
        "    # 데이터 보정(최대/최소, 자료형)\n",
        "    v_new = np.clip(v_new, 1.0, 3000.0)\n",
        "    v_new = v_new.astype(int)\n",
        "\n",
        "    return v_new\n",
        "\n",
        "def make_time_warping(v_data):\n",
        "    v_new = np.array(v_data, dtype=float)\n",
        "    data_len = len(v_new)\n",
        "\n",
        "    # 배속 설정\n",
        "    scale = pyrandom.uniform(0.8, 1.2)\n",
        "    new_len = int(data_len * scale)\n",
        "\n",
        "    x_axis = np.arange(data_len)\n",
        "    x_axis_new = np.linspace(0, data_len - 1, new_len)\n",
        "    v_scaled = np.interp(x_axis_new, x_axis, v_new)\n",
        "\n",
        "    v_final = np.zeros(data_len, dtype=float)\n",
        "\n",
        "    if new_len > data_len:\n",
        "        cut = new_len - data_len\n",
        "        start_idx = cut // 2\n",
        "        end_idx = start_idx + data_len\n",
        "\n",
        "        v_final = v_scaled[start_idx:end_idx]\n",
        "    else:\n",
        "        padding = data_len - new_len\n",
        "        padding_left = padding // 2\n",
        "        padding_right = padding - padding_left\n",
        "\n",
        "        v_final[padding_left:padding_left+new_len] = v_scaled\n",
        "        v_final[:padding_left] = v_scaled[0]\n",
        "        v_final[padding_left+new_len:] = v_scaled[-1]\n",
        "\n",
        "    v_final = v_final.astype(int)\n",
        "\n",
        "    return v_final\n",
        "\n",
        "def make_time_shift(v_data):\n",
        "    v_new = np.array(v_data, dtype=int)\n",
        "\n",
        "    shift = pyrandom.randint(-10, 10)\n",
        "\n",
        "    if shift == 0:\n",
        "        return v_new\n",
        "\n",
        "    v_final = np.zeros_like(v_new, dtype=int)\n",
        "\n",
        "    if shift > 0:\n",
        "        v_final[shift:] = v_new[:-shift]\n",
        "        v_final[:shift] = v_new[0]\n",
        "    else:\n",
        "        v_final[:shift] = v_new[-shift:]\n",
        "        v_final[shift:] = v_new[-1]\n",
        "\n",
        "    return v_final\n",
        "\n",
        "def make_scale_different(v_data):\n",
        "    v_new = np.array(v_data, dtype=float)\n",
        "\n",
        "    mean = np.mean(v_data)\n",
        "    v_new = v_new - mean\n",
        "\n",
        "    distance_new = pyrandom.uniform(1, 2.5)\n",
        "    scale_factor = (1.5 / distance_new)**2\n",
        "    v_new = v_new * scale_factor\n",
        "\n",
        "    v_new = v_new + mean\n",
        "    v_new = np.clip(v_new, 1.0, pyrandom.uniform(2950,3000))\n",
        "    v_new = v_new.astype(int)\n",
        "\n",
        "    return v_new\n",
        "\n",
        "def augment(X, factor, label):\n",
        "    n_to_add = len(X) * (factor - 1)\n",
        "    X_augmented = []\n",
        "\n",
        "    aug_functions = [make_spike_noise, make_gaussian_noise, make_time_warping, make_time_shift, make_scale_different]\n",
        "\n",
        "    for _ in range(n_to_add):\n",
        "        sample_data = X[pyrandom.randint(0, len(X) - 1)].copy()\n",
        "\n",
        "        n_to_augment = pyrandom.randint(1, 3)\n",
        "        selected_functions = pyrandom.sample(aug_functions, n_to_augment)\n",
        "\n",
        "        for func in selected_functions:\n",
        "            sample_data = func(sample_data)\n",
        "\n",
        "        X_augmented.append(sample_data)\n",
        "\n",
        "    X_augmented = np.array(X_augmented)\n",
        "    y_augmented = np.full(len(X_augmented), label, dtype=int)\n",
        "\n",
        "    return X_augmented, y_augmented\n",
        "\n",
        "def augment_dataset(X, y, normal_aug, falling_aug):\n",
        "    # 클래스별로 데이터 분리\n",
        "    X_normal = X[y == 0]\n",
        "    X_falling = X[y == 1]\n",
        "\n",
        "    X_augmented = [X]\n",
        "    y_augmented = [y]\n",
        "\n",
        "    # 증강\n",
        "    X_normal_augmented, y_normal_augmented = augment(X_normal, normal_aug, 0)\n",
        "    X_augmented.append(X_normal_augmented)\n",
        "    y_augmented.append(y_normal_augmented)\n",
        "\n",
        "    X_falling_augmented, y_falling_augmented = augment(X_falling, falling_aug, 1)\n",
        "    X_augmented.append(X_falling_augmented)\n",
        "    y_augmented.append(y_falling_augmented)\n",
        "\n",
        "    # 증강된 데이터셋 정리 및 셔플\n",
        "    X_final = np.vstack(X_augmented)\n",
        "    y_final = np.concatenate(y_augmented)\n",
        "\n",
        "    rand_idx = np.random.permutation(len(X_final))\n",
        "    X_final = X_final[rand_idx]\n",
        "    y_final = y_final[rand_idx]\n",
        "\n",
        "    return X_final, y_final"
      ],
      "metadata": {
        "cellView": "form",
        "id": "icbjWm21kSM5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 메인 코드\n",
        "# 데이터셋 로드\n",
        "X_all, y_all = load_dataset(FILE_PATH)\n",
        "\n",
        "# 5겹 K-폴드 준비\n",
        "k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "\n",
        "# 결과 저장용\n",
        "history_results = []\n",
        "metrics_results = {\"acc\": [], \"auc\": [], \"recall\": [], \"f1\": []}\n",
        "cm_results = np.zeros((2, 2))  # 혼동행렬\n",
        "\n",
        "# K-Fold 시작\n",
        "print(\"\\n5-Fold 검증 시작...\")\n",
        "\n",
        "for fold, (train_idx, test_idx) in tqdm(enumerate(k_fold.split(X_all, y_all))):\n",
        "    print(f\"\\n========== Fold {fold + 1} / 5 ==========\")\n",
        "\n",
        "    # 데이터셋 분할\n",
        "    X_train = X_all[train_idx]\n",
        "    y_train = y_all[train_idx]\n",
        "    X_test = X_all[test_idx]\n",
        "    y_test = y_all[test_idx]\n",
        "\n",
        "    X_train, y_train, X_val, y_val = split_balanced_val(X_train, y_train, 0.1)\n",
        "    print(f\"  - Split 결과:\")\n",
        "    print(f\"    > Train: {len(y_train)}\")\n",
        "    print(f\"    > Val: {len(y_val)}\")\n",
        "    print(f\"    > Test: {len(y_test)}\")\n",
        "\n",
        "    # Train 데이터 증강\n",
        "    print(\"  - Train 데이터 증강 중...\")\n",
        "    X_train_aug, y_train_aug = augment_dataset(X_train, y_train, NORMAL_AUG, FALLING_AUG)\n",
        "\n",
        "    # 데이터 전처리 및 스케일링\n",
        "    X_train_feature = create_feature(X_train_aug)\n",
        "    X_val_feature = create_feature(X_val)\n",
        "    X_test_feature = create_feature(X_test)\n",
        "\n",
        "    X_train_scaled, X_val_scaled, X_test_scaled, scaler = scale(X_train_feature, X_val_feature, X_test_feature)\n",
        "\n",
        "    print(\"  - 모델 학습 시작...\")\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_AUC', mode='max', patience=EARLY_STOPPING_PATIENCE,\n",
        "                                                  restore_best_weights=True, verbose=0)\n",
        "    model = build_model()\n",
        "    history = model.fit(\n",
        "        X_train_scaled, y_train_aug,\n",
        "        validation_data=(X_val_scaled, y_val),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # 모델 평가\n",
        "    history_results.append(history.history)\n",
        "\n",
        "    y_pred_prob = model.predict(X_test_scaled, verbose=0).ravel()\n",
        "    y_pred = (y_pred_prob >= THRESHOLD).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_prob)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    metrics_results[\"acc\"].append(acc)\n",
        "    metrics_results[\"auc\"].append(auc)\n",
        "    metrics_results[\"recall\"].append(recall)\n",
        "    metrics_results[\"f1\"].append(f1)\n",
        "    cm_results += confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f\"  >> Fold {fold + 1}: ACC={acc:.4f}, AUC={auc:.4f}\\n\")\n",
        "\n",
        "# 데이터 시각화 부분은 AI의 도움을 받았습니다\n",
        "min_epochs = min([len(h['loss']) for h in history_results])\n",
        "epochs_range = range(1, min_epochs + 1)\n",
        "\n",
        "avg_loss = np.mean([h['loss'][:min_epochs] for h in history_results], axis=0)\n",
        "avg_val_loss = np.mean([h['val_loss'][:min_epochs] for h in history_results], axis=0)\n",
        "avg_acc = np.mean([h['accuracy'][:min_epochs] for h in history_results], axis=0)\n",
        "avg_val_acc = np.mean([h['val_accuracy'][:min_epochs] for h in history_results], axis=0)\n",
        "avg_auc = np.mean([h['AUC'][:min_epochs] for h in history_results], axis=0)\n",
        "avg_val_auc = np.mean([h['val_AUC'][:min_epochs] for h in history_results], axis=0)\n",
        "\n",
        "avg_cm = cm_results / 5.0\n",
        "labels = ['Accuracy', 'AUC', 'Recall', 'F1-Score']\n",
        "means = [np.mean(metrics_results[k]) for k in ['acc', 'auc', 'recall', 'f1']]\n",
        "stds = [np.std(metrics_results[k]) for k in ['acc', 'auc', 'recall', 'f1']]\n",
        "\n",
        "# 2. ★ 화면 레이아웃 잡기 (여기가 핵심!) ★\n",
        "fig = plt.figure(figsize=(18, 12))  # 전체 창 크기 넉넉하게\n",
        "gs = fig.add_gridspec(2, 6)  # 2행 6열로 바둑판 쪼개기\n",
        "\n",
        "# --- [상단 1행] 3개 그래프 (각각 2칸씩 차지) ---\n",
        "ax1 = fig.add_subplot(gs[0, 0:2])  # 0~2칸\n",
        "ax1.plot(epochs_range, avg_loss, 'b-', label='Train')\n",
        "ax1.plot(epochs_range, avg_val_loss, 'r--', label='Val')\n",
        "ax1.set_title('Average Loss')\n",
        "ax1.legend();\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2 = fig.add_subplot(gs[0, 2:4])  # 2~4칸\n",
        "ax2.plot(epochs_range, avg_acc, 'b-', label='Train')\n",
        "ax2.plot(epochs_range, avg_val_acc, 'r--', label='Val')\n",
        "ax2.set_title('Average Accuracy')\n",
        "ax2.legend();\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "ax3 = fig.add_subplot(gs[0, 4:6])  # 4~6칸\n",
        "ax3.plot(epochs_range, avg_auc, 'b-', label='Train')\n",
        "ax3.plot(epochs_range, avg_val_auc, 'r--', label='Val')\n",
        "ax3.set_title('Average AUC')\n",
        "ax3.legend();\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# --- [하단 2행] 2개 그래프 (각각 3칸씩 차지 -> 큼직하게!) ---\n",
        "# 1. 혼동 행렬 (왼쪽 절반)\n",
        "ax4 = fig.add_subplot(gs[1, 0:3])  # 0~3칸\n",
        "sns.heatmap(avg_cm, annot=True, fmt='.1f', cmap='Purples', ax=ax4, annot_kws={\"size\": 14},  # 글자도 좀 키움\n",
        "            xticklabels=['Pred Normal', 'Pred Fall'],\n",
        "            yticklabels=['Act Normal', 'Act Fall'])\n",
        "ax4.set_title('Average Confusion Matrix', fontsize=14)\n",
        "ax4.set_ylabel('True Label')\n",
        "ax4.set_xlabel('Predicted Label')\n",
        "\n",
        "# 2. 성능 지표 막대 (오른쪽 절반)\n",
        "ax5 = fig.add_subplot(gs[1, 3:6])  # 3~6칸\n",
        "bars = ax5.bar(labels, means, yerr=stds, capsize=5, color='skyblue', edgecolor='black')\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax5.text(bar.get_x() + bar.get_width() / 2., height + 0.01,\n",
        "             f'{height:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "ax5.set_title('Final Performance Metrics', fontsize=14)\n",
        "ax5.set_ylim(0, 1.15)\n",
        "ax5.grid(axis='y', linestyle='--', alpha=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "RQTEuhmtkvIu",
        "outputId": "d89b92bb-6b33-4145-ea88-653b21eeeb20"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5-Fold 검증 시작...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Fold 1 / 5 ==========\n",
            "  - Split 결과:\n",
            "    > Train: 440\n",
            "    > Val: 48\n",
            "    > Test: 122\n",
            "  - Train 데이터 증강 중...\n",
            "  - 모델 학습 시작...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [01:22, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1386892006.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m                                                   restore_best_weights=True, verbose=0)\n\u001b[1;32m     45\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_aug\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             ):\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    917\u001b[0m           )\n\u001b[1;32m    918\u001b[0m       )\n\u001b[0;32m--> 919\u001b[0;31m       return self._concrete_variable_creation_fn._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    920\u001b[0m           \u001b[0mfiltered_flat_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_variable_creation_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1689\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}